# ab-testing-MobileGame

This project analyzes an A/B test from a mobile game (Cookie Cats) to evaluate whether **moving the first gate from level 30 to level 40** affects player engagement and retention. The notebook walks through data loading, EDA, statistical testing (proportions and means), and two complementary inference approaches: **delta (twoâ€‘proportion zâ€‘test)** and **bootstrap**.

> **Key insight**
>
> *Dayâ€‘1 retention* shows **no statistically significant difference** between variants, while *Dayâ€‘7 retention* shows a **statistically significant** difference between variants under the bootstrap analysis (95% CI does **not** include 0).

---

## ğŸ“¦ Dataset

The data contains userâ€‘level records with at least the following fields (inferred from notebook code and EDA):

- `version` â€” experiment group indicator: **A** (control, gate at level 30) vs **B** (treatment, gate at level 40)
- `sum_gamerounds` â€” total gameplay rounds in the observation window
- `retention_1` â€” indicator for returning on **Dayâ€‘1** after install (0/1)
- `retention_7` â€” indicator for returning on **Dayâ€‘7** after install (0/1)
- (Other helper fields in SQL summaries: `label`, `cnt`, `game`, `userid`)

Data are loaded in the notebook (via `gdown`/CSV or from a small `sqlite3` demo).

---

## ğŸ¯ Experiment Design

- **Objective:** Does moving the first â€œgateâ€ from level 30 â†’ 40 improve engagement/retention?
- **Primary metrics:** Binary indicators`retention_1` (Dayâ€‘1) and `retention_7` (Dayâ€‘7)
- **Secondary metric:** `sum_gamerounds` - Total gameplay rounds
- **Composite Metric:** Combined Day1 and Day7 rentention
- **Hypotheses (for retention):**
  - H0: p_A = p_B
  - H1: p_A â‰  p_B (twoâ€‘sided)

---

## ğŸ” Methods

### 1) Exploratory Data Analysis (EDA)
- Inspect distributions and summary statistics for `sum_gamerounds`, `retention_1`, `retention_7`
- Compare A vs B across metrics (tables and plots)

### 2) Inference for Proportions (Retention)
- **Delta method / twoâ€‘proportion zâ€‘test**: pooledâ€‘variance normal approximation for \(p_1 - p_2\).
- **Bootstrap**: nonâ€‘parametric resampling to form the sampling distribution of the difference in means for `retention_1` and `retention_7`, with a 95% CI.

**Reported bootstrap CIs (from the notebook):**
- **Dayâ€‘1 retention:** 95% CI â‰ˆ **[-0.002, 0.012]** â†’ *not significant* (CI includes 0)
- **Dayâ€‘7 retention:** 95% CI â‰ˆ **[0.008, 0.022]** â†’ *significant* (CI excludes 0)

### 3) Inference for Means (Engagement)
- Compare `sum_gamerounds` between A/B (visualization + summary; parametric or nonâ€‘parametric test options depending on distributional checks)

> The notebook also contrasts **parametric** vs **nonâ€‘parametric** choices (e.g., when normality/variance assumptions are shaky).

---

## âœ… Results (at a glance)

- **Dayâ€‘1 retention:** No statistically significant difference between A and B.
- **Dayâ€‘7 retention:** Statistically significant difference between A and B (bootstrap 95% CI excludes 0).
- **Gameplay rounds (`sum_gamerounds`):** Similar at high level; see notebook for plots and group summaries.

**Interpretation:** Moving the first gate to level 40 **did not hurt early (Dayâ€‘1) retention** and shows **meaningful change by Dayâ€‘7**. This suggests player experience is not negatively impacted in the short term and might support better mediumâ€‘term engagement/retention dynamics.

---

## ğŸ§± Repository Structure

```
â”œâ”€â”€ A_B_Testing_ã€Pythonã€‘.ipynb     # Main analysis notebook (EDA + tests + bootstrap)
â”œâ”€â”€ README.md                        # You are here
â””â”€â”€ data/                            # (Optional) If you store CSV locally
```

> The notebook also includes small **SQLite** snippets for aggregate summaries.

---

## ğŸ› ï¸ Environment & Dependencies

The notebook uses the following major libraries (as detected in code):
- `pandas`, `numpy`
- `matplotlib`, `seaborn`
- `scipy`, `statsmodels`
- `sqlite3`, `gdown`
- `tqdm`, `IPython.display`

Install (minimal):
```bash
pip install pandas numpy matplotlib seaborn scipy statsmodels tqdm gdown
```

---

## ğŸš€ How to Run

1. Clone the repo and open the notebook:
   ```bash
   git clone https://github.com/yourusername/ab-testing-cookiecats.git
   cd ab-testing-cookiecats
   jupyter notebook A_B_Testing_ã€Pythonã€‘.ipynb
   ```
2. (If pulling from Drive) Ensure the download step (via `gdown`) succeeds or place the CSV locally and update the load path.
3. Run cells in order; figures and tables will be generated inline.

---

## ğŸ“š Notes & Best Practices

- Ensure test horizon and **sample size** are sufficient; avoid peeking too early.
- Prefer **twoâ€‘sided** tests unless you have strong prior reason to use oneâ€‘sided.
- For binary metrics and large samples, twoâ€‘proportion **zâ€‘test** is standard; **bootstrap** is a robust nonâ€‘parametric crossâ€‘check.
- Report **effect sizes** with **confidence intervals**, not only pâ€‘values.
- Guard against multipleâ€‘testing if analyzing many metrics/segments.

---

## ğŸ§­ Roadmap (Optional Enhancements)

- Add a minimal **CLI** to reproduce the bootstrap analysis from CSV
- Add **power & sampleâ€‘size** calculator for retention deltas
- Extend to **sequential testing** or **Bayesian** A/B analysis
- Automate **data validation** and **outlier handling**

---

## ğŸ“œ License

